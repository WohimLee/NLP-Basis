# Masked Multi-Head Attention


## 1 工作原理
在处理序列生成任务时，比如语言模型预测下一个单词，模型在生成每个单词时只能利用之前已经生成的单词，而不能看到未来的单词。为了实现这一点，引入了 Masked Multi-Head Attention 机制。

## 2 具体步骤
### 2.1 输入序列处理
输入序列通过嵌入层（embedding layer）和位置编码（positional encoding）被转换为向量表示。


### 2.2 自注意力计算
计算注意力分数（attention scores）时，模型会对每个单词位置计算 Query、Key 和 Value。

计算得到的注意力分数矩阵会乘以一个 mask 矩阵，该矩阵通常是一个上三角矩阵，主对角线以下为0，以上为负无穷（用一个极小值来表示）。这个 mask 确保在计算某个单词的表示时，只考虑该单词前面的单词，而屏蔽掉后面的单词（即屏蔽掉不应该看到的未来信息）。

### 2.3 多头注意力
多个头部并行计算，然后将每个头的结果拼接起来，通过一个线性变换映射回到原来的维度。
### 2.4 输出
Masked Multi-Head Attention 的输出可以作为下一层的输入，或者用于生成下一个单词
## 3 总结
Masked Multi-Head Attention 的关键在于它通过遮盖（masking）未来的单词，确保在自回归模型中，每个单词的预测只依赖于它前面的单词，而不会泄露未来的信息。这对于生成式任务至关重要，能够保持生成过程的合理性和连贯性。