# RNN
## 1 Intro


## 2 Data Flow
RNN（循环神经网络）的数据流可以分为几个关键步骤：
- 输入处理：检查输入类型和维度，解包或调整输入张量。
- 隐藏状态初始化：根据输入和批次情况初始化或调整隐藏状态。
- 前向传播计算：使用指定的非线性激活函数（tanh或ReLU）进行前向传播计算，生成输出和新的隐藏状态。
- 输出处理：根据输入类型（是否为PackedSequence）对输出进行适当处理并返回

#### 2.1 输入处理

RNN接受一个序列数据作为输入, 每个时间步都有一个输入向量。假设输入序列长度为 $\mathrm{T}$, 输入维度为 $D$ 
- 输入数据的形状：`(T, D)` (单个样本) 或 `(N, T, D)`（批量样本，其中 $N$ 是批量大小）。
#### 2.2 初始化隐藏状态

在第一个时间步, RNN需要一个初始的隐藏状态。如果没有提供，通常会初始化为全零向量。
- 隐藏状态的形状：`(H)`（单个样本）或 `(N, H)`（批量样本, 其中H是隐藏层的维度）。
#### 2.3 时间步循环

RNN通过循环处理输入序列中的每个时间步。在每个时间步, RNN会更新隐藏状态并生成一个输出。
- 对于每个时间步 $t$, 输入向量为 ‘x_t`, 隐藏状态为 `h_t`。

以下是每个时间步的详细步骤：

##### 2.3.1 计算隐藏状态

隐藏状态 h_t`由当前输入 x_t`和前一个时间步的隐藏状态`h_\{t-1\}`共同决定。常见的计算方式是通过一个线性变换和激活函数:
$$
h_t=\tanh \left(W_{x h} x_t+W_{h h} h_{t-1}+b_h\right)
$$
- $W_{xh}$: 是输入到隐藏层的权重矩阵
- $W_{hh}$: 是隐藏层到隐藏层的权重矩阵
- $b_h$: 是隐藏层的偏置向量
##### 2.3.2 生成输出
$$
y_t=W_{h y} h_t+b_y
$$
- `W_\{hy\}`是隐藏层到输出层的权重矩阵。
- `b_y`是输出层的偏置向量。
#### 2.4 隐藏状态传递

在时间步 ‘t`完成后, 更新后的隐藏状态`h_t`将传递到下一个时间步`t+1`。这个过程会持续到序列的最后一个时间步。
#### 2.5 最终输出

RNN可以在每个时间步生成一个输出, 也可以在处理完整个序列后仅生成最后一个时间步的输出, 这取决于具体的任务需求（例如序列到序列的任务或者序列到单个输出的任务）。