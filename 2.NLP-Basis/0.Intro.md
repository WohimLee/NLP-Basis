# Intro
## 1 NLP 基本任务
- 文本分类（Text Classification）
    - 情感分析（Sentiment Analysis）：判断文本的情感倾向（如正面、负面或中性）
    - 垃圾邮件检测（Spam Detection）：区分垃圾邮件和正常邮件
    - 主题分类（Topic Classification）：将文本分类到不同的主题类别
- 命名实体识别（Named Entity Recognition, NER）：识别文本中具有特定意义的实体（如人名、地名、机构名等）
- 词性标注（Part-of-Speech Tagging, POS Tagging）：给每个单词标注其词性（如名词、动词、形容词等）
- 语法解析（Parsing）：分析句子的语法结构，生成句法树或依存树
- 机器翻译（Machine Translation）：将文本从一种语言翻译成另一种语言
- 问答系统（Question Answering, QA）：从文本中自动回答用户的问题
- 文本生成（Text Generation）：自动生成符合语法和语义的自然语言文本（如对话生成、文章生成等）
- 摘要生成（Summarization）：自动生成文本的摘要，分为抽取式摘要和生成式摘要
- 核心指代消解（Coreference Resolution）：识别文本中指代关系，如确定代词（如“他”、“她”）指代的实体
- 情感和情绪分析（Emotion Recognition and Sentiment Analysis）：识别和分析文本中表达的情感和情绪
- 语音识别（Speech Recognition）：将语音信号转换为对应的文本
- 对话系统（Dialogue Systems）：设计和实现能与用户进行自然对话的系统，包括聊天机器人和智能助理
- 信息检索（Information Retrieval）：从大量数据中检索相关信息（如搜索引擎）
- 文本相似度计算（Text Similarity）：计算两段文本之间的相似度，应用于推荐系统、抄袭检测等领域
- 语言建模（Language Modeling）：预测句子中下一个词或生成合理的文本序列

## 2 词向量
在 NLP 任务中，首先要考虑 `词` 如何在计算机中表示

词向量（Word Vector）是自然语言处理中一种表示单词的方法。它将单词映射到一个连续向量空间中，使得具有相似意义的词在向量空间中彼此接近。词向量的引入和应用极大地提升了自然语言处理任务的效果。

### 2.1 词向量的基本概念
- 高维向量表示：每个单词被表示为一个固定长度的向量，这些向量通常是高维的（例如，100维、300维等）。
- 语义相似性：在词向量空间中，相似意义的词向量之间的距离较近。例如，“国王”和“王后”的词向量应该比“国王”和“桌子”的词向量距离更近。

### 2.2 词向量的常见训练方法
>Word2Vec
- CBOW（Continuous Bag of Words）：通过上下文预测中心词。
- Skip-gram：通过中心词预测上下文词。

Word2Vec是由Google提出的，它通过神经网络模型将词映射到向量空间中

>GloVe（Global Vectors for Word Representation）：
- 由斯坦福大学提出，利用全局词共现矩阵，通过矩阵分解的方法获得词向量

>FastText
- 由Facebook提出的扩展模型，不仅考虑单词本身，还考虑单词的子词（n-gram），因此可以处理未登录词（OOV）

>BERT（Bidirectional Encoder Representations from Transformers）
- 由 Google 提出的基于 Transformer 的模型，生成上下文敏感的词向量，即同一个词在不同的上下文中可以有不同的向量表示

