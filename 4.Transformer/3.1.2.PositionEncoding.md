
# Positional Encoding
- [LLM时代Transformer中的Positional Encoding](https://zhuanlan.zhihu.com/p/664214907)

Transformer模型中的Positional Encoding（位置编码）是用于向序列中的每个词（token）注入位置信息的机制。这是因为Transformer模型本身是基于自注意力（self-attention）机制的，而自注意力机制是完全不考虑序列中词的顺序的。换句话说，Transformer对输入序列的词的顺序是“无感知”的，因此需要一种方法来提供这种顺序信息。Positional Encoding就是为了解决这个问题而引入的。



<div align=center>
    <image src="imgs/positionalEncoding.png" width="800">
</div>

<div align=center>
    <image src="imgs/pe2.png" width="800">
</div>

## 1 为什么需要Positional Encoding？
在传统的循环神经网络（RNN）和卷积神经网络（CNN）中，模型可以通过其结构自然地捕捉到输入序列的顺序信息。例如，RNN的隐状态是依赖于前面时刻的状态，CNN通过卷积核滑动捕捉局部的顺序信息。然而，Transformer模型采用的是完全基于自注意力的机制，它可以在全局范围内直接进行信息的交互，这样的结构虽然非常强大，但也失去了序列的顺序感。

为了让Transformer模型能够识别输入序列中各个词的位置，Positional Encoding被引入到模型的输入中。这一编码会与输入的词向量相加，使得每个词不仅携带有其本身的语义信息，还包含了其在序列中的位置信息。


## 2 Positional Encoding的定义
通常，Positional Encoding被设计成如下形式：

对于输入序列中的第 $\operatorname{pos}$ 位置和第 $i$ 维度的编码, Positional Encoding可以表示为：

$$
\begin{gathered}
P E_{(p o s, 2 i)}=\sin \left(\frac{p o s}{10000^{2 i / d_{\text {model }}}}\right) \\
P E_{(p o s, 2 i+1)}=\cos \left(\frac{p o s}{10000^{2 i / d_{\text {model }}}}\right)
\end{gathered}
$$

其中, $d$ 是词向量的维度, $p o s$ 表示词在序列中的位置, $i$ 是词向量的维度索引, 通过将这些位置编码与输入嵌入相加，模型能够"感知"到输入序列中词语的顺序。



## 3 Positional Encoding的特点
>平滑性和周期性
- 通过使用正弦和余弦函数，位置编码具有平滑的周期性变化。这种设计使得模型能够方便地推断相对位置以及捕捉位置信息。

>维度的不同变化速率
- 在不同维度上，正弦和余弦函数的频率不同，这样的设计保证了模型能够从不同的维度捕获位置信息。

>无参数化
- 位置编码是固定的, 不会在训练过程中更新。这与那些使用可学习参数的位置信息注入方法不同

## 4 Positional Encoding的使用

在 Transformer 模型中，Positional Encoding 会被直接加到输入的词向量上。这意味着每个词的表示不仅包含其语义信息，还包含了它在句子中的位置信息。由于这一操作是简单的加法，因此模型可以直接将其作为一种信号来学习如何利用这些位置信息进行序列建模。

>过程
- 位置编码向量被加到输入嵌入向量上，从而使模型能够识别每个词在序列中的相对或绝对位置。
- 在原始的 Transformer 模型中，位置编码是通过正弦和余弦函数生成的。这些函数的频率在不同的维度上有所变化，从而能够为每个位置生成唯一的编码。



## 5 总结

Positional Encoding为Transformer模型提供了必要的位置信息，使得模型能够识别输入序列中各个词的位置。通过正弦和余弦函数的编码方式，这种位置信息能够以一种固定且有效的方式融入模型中，从而补充自注意力机制的不足，确保模型能够更好地处理顺序相关的任务。

## 6 Code

>PositionalEncoding
```py
class PositionalEncoding(nn.Module):

    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:
        super().__init__()
        self.d_model = d_model
        self.seq_len = seq_len
        self.dropout = nn.Dropout(dropout) # 正则手段, 防止过拟合
        # Create a matrix of shape (seq_len, d_model)
        pe = torch.zeros(seq_len, d_model)
        # Create a vector of shape (seq_len)
        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1) # (seq_len, 1)
        # Create a vector of shape (d_model)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # (d_model / 2)
        # Apply sine to even indices
        pe[:, 0::2] = torch.sin(position * div_term) # sin(position * (10000 ** (2i / d_model))
        # Apply cosine to odd indices
        pe[:, 1::2] = torch.cos(position * div_term) # cos(position * (10000 ** (2i / d_model))
        # Add a batch dimension to the positional encoding
        pe = pe.unsqueeze(0) # (1, seq_len, d_model)
        # Register the positional encoding as a buffer
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False) # (batch, seq_len, d_model) 在训练过程不学习，之后的其它模型改了
        return self.dropout(x)
```