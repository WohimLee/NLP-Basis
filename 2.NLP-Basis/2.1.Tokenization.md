
# Tokenization
- Tiktokenizer: https://tiktokenizer.vercel.app/

Tokenization（分词）是自然语言处理（NLP）中的一个重要步骤，涉及将文本分解为更小的单元，通常是单词、子词或字符，这些单元被称为“tokens”（标记）。

Tokenization 在 NLP 中是一个重要的前处理步骤，许多现代NLP模型，如BERT、GPT等，都依赖于有效的 tokenization 方法来处理输入文本。

>Tokenizer 本质
- 在 UTF-8 下，按照概率，经常一起出现的数字（汉字对应），作为一个新的token


## 1 Tokenization 与 Word2Vec
Tokenization 和 Word2Vec 之间有着紧密的关联，主要体现在它们在自然语言处理中的作用和工作流程上。


Tokenization（分词） 是将一段文本划分为独立的单词或子词的过程，这是大多数自然语言处理任务中的第一步。在中文处理时，通常使用像 Jieba 这样的分词工具来进行分词，将一句话分解成若干个单词。

在 Word2Vec 中，分词后的文本用于构建词汇表和训练词向量。分词后的每个单词（token）将被视为模型的输入，用来学习单词之间的语义关系。

Word2Vec 的两种模型（CBOW 和 Skip-Gram）依赖于 token 作为输入来训练上下文关系，从而生成具有语义信息的词向量。

如果分词的质量不佳，比如词汇边界不清晰或错误分词，会影响 Word2Vec 模型的训练效果。模型可能会学到不准确的词语语义关系，进而影响词向量的表现。
特别是在中文中，合理的分词对于捕捉词语之间的语义依赖关系非常重要。


## 2 Tokenization 的方法

>基于空格的分词
- 这种方法适用于像英语这样的语言。它将文本按空格分隔，直接将单词作为token。
- 示例："Tokenization is important." → ["Tokenization", "is", "important", "."]

>基于正则表达式的分词
- 通过使用正则表达式可以处理标点符号、数字等，能够更灵活地定义token的边界。
- 示例："It's important, isn't it?" → ["It", "'s", "important", ",", "isn", "'t", "it", "?"]

>子词（Subword）分词
- 这种方法在处理像德语、芬兰语等存在复合词的语言时尤为有效。常见的子词分词算法包括BPE（Byte Pair Encoding）和WordPiece。
- 示例："unhappiness" → ["un", "##happiness"] (WordPiece)

>字符级别的分词
- 直接将文本分解为单个字符。这种方法常用于处理无法预测的词汇或极为细粒度的文本分析。
- 示例："hello" → ["h", "e", "l", "l", "o"]

>中文分词
- 中文由于没有明显的单词边界，通常使用基于统计或规则的方法进行分词，如Jieba分词、THULAC等。
- 示例："自然语言处理" → ["自然", "语言", "处理"]



## 3 常见特殊标记
>[PAD]（Padding Token）
- 用途: 用于对齐（padding）不同长度的输入或输出序列。
- 解释: 在处理批量输入时，各个句子的长度可能不同，为了形成统一的矩阵输入，较短的句子会被填充上 [PAD] 标记，使得所有句子达到相同的长度。这在深度学习模型中非常常见，尤其是批处理模式下。
- 示例:
    ```
    原始句子: ["我", "爱", "自然", "语言"]
    填充后: ["我", "爱", "自然", "语言", "[PAD]", "[PAD]"]
    ```

>[UNK]（Unknown Token）
- 用途: 表示在词汇表中未找到的词或字。
- 解释: 当模型遇到词汇表中没有的词（通常称为未知词）时，会使用 [UNK] 来代替。这对于词汇表较小的模型尤其重要，确保模型在处理未知词时仍能正常运行。
- 示例:
    ```
    输入句子: ["我", "爱", "NLP"] （假设 "NLP" 不在词汇表中）
    处理后: ["我", "爱", "[UNK]"]
    ```

>[CLS]（Classification Token）
- 用途: 用于句子分类任务，通常在模型输入的最前面。
- 解释: [CLS] 标记通常作为输入序列的第一个标记，代表整个句子的摘要信息。在句子分类任务中，模型的最后一层输出中与 [CLS] 对应的向量常用于分类决策。
- 示例:
    ```
    输入序列: [CLS] 我 爱 自然 语言
    用途: 句子分类任务中的输入。
    ```


>[SEP]（Separator Token）
- 用途: 用于分隔不同的句子或段落，尤其在处理双句子任务时。
- 解释: [SEP] 标记用于分隔两个句子，在诸如自然语言推理（NLI）、问答（QA）等任务中，用来区分前后两个句子或段落。在 BERT 等模型中，两个句子通常会被连接在一起，中间用 [SEP] 分隔。
- 示例:
```
输入: [CLS] 句子一 [SEP] 句子二 [SEP]
用途: 用于任务如句子配对、问答。
```

>[MASK]（Masking Token）
- 用途: 用于掩盖（mask）部分输入词汇，常用于掩蔽语言模型（如 BERT）的预训练阶段。
- 解释: 在 BERT 的预训练过程中，模型会随机将输入中的一些词替换为 [MASK] 标记，要求模型预测出这些被遮蔽的词。这种训练方式帮助模型学习更好的上下文表示。
- 示例:
    ```
    原始句子: ["我", "爱", "自然", "语言"]
    遮蔽后: ["我", "爱", "[MASK]", "语言"]
    目标: 预测 [MASK] 为 "自然"。
    ```
>[SOS]（Start of Sentence Token）
- 用途: 标示句子的开始，常用于序列到序列模型的解码阶段。
- 解释: [SOS] 用于指示生成的开始，在翻译、文本生成等任务中，解码器通常从 [SOS] 标记开始生成句子，直到生成结束标记 [EOS]。
- 示例:
    ```
    目标序列: [SOS] 你 好 吗 [EOS]
    用途: 用于生成第一个词。
    ```

>[EOS]（End of Sentence Token）
- 用途: 标示句子的结束，常与 [SOS] 配合使用。
- 解释: [EOS] 用于指示句子的结束，模型在生成或处理序列时，遇到 [EOS] 标记通常表示停止处理该序列。这在文本生成、翻译任务中尤为重要，避免生成过长的序列。
- 示例:
    ```
    目标序列: [SOS] 你 好 吗 [EOS]
    用途: 指示生成结束。
    ```