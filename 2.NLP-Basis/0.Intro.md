# Intro
## 1 NLP 基本任务
- 文本分类（Text Classification）
    - 情感分析（Sentiment Analysis）: 判断文本的情感倾向（如正面、负面或中性）
    - 垃圾邮件检测（Spam Detection）: 区分垃圾邮件和正常邮件
    - 主题分类（Topic Classification）: 将文本分类到不同的主题类别
- 命名实体识别（Named Entity Recognition, NER）: 识别文本中具有特定意义的实体（如人名、地名、机构名等）
- 词性标注（Part-of-Speech Tagging, POS Tagging）: 给每个单词标注其词性（如名词、动词、形容词等）
- 语法解析（Parsing）: 分析句子的语法结构，生成句法树或依存树
- 机器翻译（Machine Translation）: 将文本从一种语言翻译成另一种语言
- 问答系统（Question Answering, QA）: 从文本中自动回答用户的问题
- 文本生成（Text Generation）: 自动生成符合语法和语义的自然语言文本（如对话生成、文章生成等）
- 摘要生成（Summarization）: 自动生成文本的摘要，分为抽取式摘要和生成式摘要
- 核心指代消解（Coreference Resolution）: 识别文本中指代关系，如确定代词（如“他”、“她”）指代的实体
- 语音识别（Speech Recognition）: 将语音信号转换为对应的文本
- 对话系统（Dialogue Systems）: 设计和实现能与用户进行自然对话的系统，包括聊天机器人和智能助理
- 信息检索（Information Retrieval）: 从大量数据中检索相关信息（如搜索引擎）
- 文本相似度计算（Text Similarity）: 计算两段文本之间的相似度，应用于推荐系统、抄袭检测等领域
- 语言建模（Language Modeling）: 预测句子中下一个词或生成合理的文本序列

## 2 Tokenization
- Tiktokenizer: https://tiktokenizer.vercel.app/
Tokenization（分词）是自然语言处理（NLP）中的一个重要步骤，涉及将文本分解为更小的单元，通常是单词、子词或字符，这些单元被称为“tokens”（标记）。这个过程在许多NLP任务中都是基础性的

### 2.1 Tokenization 的方法

>基于空格的分词
- 这种方法适用于像英语这样的语言。它将文本按空格分隔，直接将单词作为token。
- 示例："Tokenization is important." → ["Tokenization", "is", "important", "."]

>基于正则表达式的分词
- 通过使用正则表达式可以处理标点符号、数字等，能够更灵活地定义token的边界。
- 示例："It's important, isn't it?" → ["It", "'s", "important", ",", "isn", "'t", "it", "?"]

>子词（Subword）分词
- 这种方法在处理像德语、芬兰语等存在复合词的语言时尤为有效。常见的子词分词算法包括BPE（Byte Pair Encoding）和WordPiece。
- 示例："unhappiness" → ["un", "##happiness"] (WordPiece)

>字符级别的分词
- 直接将文本分解为单个字符。这种方法常用于处理无法预测的词汇或极为细粒度的文本分析。
- 示例："hello" → ["h", "e", "l", "l", "o"]

>中文分词
- 中文由于没有明显的单词边界，通常使用基于统计或规则的方法进行分词，如Jieba分词、THULAC等。
- 示例："自然语言处理" → ["自然", "语言", "处理"]

### 2.2 Tokenization的挑战
Tokenization并非总是简单直观的，尤其在以下情况下：
- 多义性：一个序列可能有多种分词方式，如中文中的“长大”和“长江大桥”，需要根据上下文选择正确的分词方法。
- 新词和拼写错误：模型可能无法识别从未见过的词或拼写错误的词，这时子词或字符级别的分词方法通常更为有效。
- 标点符号：如何处理标点符号在不同应用中可能需要不同的策略，有时标点符号被视为独立的token，有时被忽略。

### 2.3 应用
Tokenization 在 NLP 中是一个重要的前处理步骤，许多现代NLP模型，如BERT、GPT等，都依赖于有效的 tokenization 方法来处理输入文本。

### 2.4 Tokenizer 本质
在 UTF-8 下，按照概率，经常一起出现的数字（汉字对应），作为一个新的token




## 3 词向量
在 NLP 任务中，首先要考虑 `词` 如何在计算机中表示

词向量（Word Vector）是 NLP 的一种表示单词的方法。它将单词映射到一个连续向量空间中，使得具有相似意义的词在向量空间中彼此接近。词向量的引入和应用极大地提升了自然语言处理任务的效果。


### 3.1 词向量的基本概念
- 高维向量表示: 每个单词被表示为一个固定长度的向量，这些向量通常是高维的（例如，100维、300维等）。
- 语义相似性: 在词向量空间中，相似意义的词向量之间的`距离`较近。例如，“国王”和“王后”的词向量应该比“国王”和“桌子”的词向量`距离`更近。

### 3.2 词向量的常见训练方法
>Word2Vec
- Word2Vec是由Google提出的，它通过神经网络模型将词映射到向量空间中
    - CBOW（Continuous Bag of Words）: 通过上下文预测中心词。
    - Skip-gram: 通过中心词预测上下文词。

>BERT（Bidirectional Encoder Representations from Transformers）
- 由 Google 提出的基于 Transformer 的模型，生成上下文敏感的词向量，即同一个词在不同的上下文中可以有不同的向量表示

