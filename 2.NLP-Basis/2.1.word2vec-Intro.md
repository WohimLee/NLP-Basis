
# word2vec
Word2Vec 是一种用于生成词向量（Word Vectors）的技术，由 Google 的 Tomas Mikolov 等人在 2013 年提出。它通过浅层神经网络将单词映射到一个连续向量空间中，使得语义上相似的词在该空间中彼此接近。Word2Vec 主要有两种模型：
- 连续词袋模型（CBOW）
- 跳字模型（Skip-gram）

word2vec的重要假设：文本中离的越近的词，相似度越高

上下文词：给定一个窗口，例如说 2，那么上下文词就是前后一共2个词

## 1 Intro
W1 矩阵拿来用, W2 矩阵是建模产生的

## 2 两种模型

### 1.1 连续词袋模型（Continuous Bag of Words, CBOW）
>工作原理
- CBOW 模型通过上下文词语来预测中心词。具体来说，它将一个词的上下文（前后一定数量的词）作为输入，输出该中心词的预测结果

>具体过程
1. 给定一个中心词及其周围的上下文词
2. 使用上下文词的向量表示，通过神经网络预测中心词
3. 更新模型参数，使得模型能够更准确地预测中心词

>优点
- 训练速度快：CBOW 模型通过在一次训练过程中预测中心词，而不是多个上下文词，因此它通常训练速度较快
- 适合小数据集：由于 CBOW 模型训练速度快且计算效率高，因此在数据集较小时表现良好

例如，给定句子 "The quick brown fox jumps over the lazy dog" 和中心词 "jumps"，CBOW 模型会使用上下文词 ["The", "quick", "brown", "fox", "over", "the", "lazy", "dog"] 来预测 "jumps"。

<div align=center>
    <image src="imgs/CBOW.png" width=600>
</div>

### 1.2 跳字模型（Skip-gram）
>工作原理
- Skip-gram 模型通过中心词来预测其上下文词。具体来说，它将一个词作为输入，输出该词的上下文词的预测结果

>具体过程
1. 给定一个中心词
2. 使用中心词的向量表示，通过神经网络预测其周围的上下文词
3. 更新模型参数，使得模型能够更准确地预测上下文词

>优点
- 捕捉稀有词的关系：Skip-gram 模型在处理稀有词和不常见词对时效果更好，因为它能通过上下文更好地捕捉词语的语义关系
- 在大数据集上表现优异：Skip-gram 在大数据集上能够更好地捕捉词语之间的复杂语义关系，因此在大规模语料库上训练效果显著

例如，给定句子 "The quick brown fox jumps over the lazy dog" 和中心词 "jumps"，Skip-gram 模型会使用 "jumps" 来预测上下文词 ["The", "quick", "brown", "fox", "over", "the", "lazy", "dog"]

<div align=center>
    <image src="imgs/skip-gram.png" width=600>
</div>


### 1.3 选择模型的建议
>数据规模
- 如果数据规模较小，建议使用 CBOW 模型，因为它训练速度更快
- 如果数据规模较大，建议使用 Skip-gram 模型，因为它能更好地捕捉复杂语义关系

>任务类型
- 如果任务对稀有词的表示要求较高，建议使用 Skip-gram 模型
- 如果任务对训练速度要求较高，且数据规模较小，建议使用 CBOW 模型

### 1.4 总结
CBOW 和 Skip-gram 模型在不同的应用场景中各有优势。CBOW 适合小数据集和需要快速训练的任务，而 Skip-gram 更适合大数据集和需要捕捉复杂语义关系的任务。根据具体的应用需求和数据特点，选择合适的模型可以更好地实现目标。
