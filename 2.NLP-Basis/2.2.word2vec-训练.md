
## word2vec 模型训练
<div align=center>
    <image src="imgs/word2vec.png" width=800>
</div>
&emsp;


### 1 Word2Vec 的训练步骤
1. 语料准备：收集大量的文本数据，形成训练语料库
2. 构建词汇表：将语料库中的所有单词构建成词汇表，并为每个单词分配一个唯一的索引
3. 初始化词向量：初始化每个单词的向量表示，通常为随机值
4. 训练模型：使用 CBOW 或 Skip-gram 模型，通过梯度下降等优化算法，迭代调整词向量，使模型能够更好地预测上下文或中心词
5. 保存词向量：训练完成后，将词向量保存下来，以供后续使用

<div align=center>
    <image src="imgs/word2vec-dataflow.png" width=800>
</div>
&emsp;



>优点
- 捕捉语义关系：Word2Vec 能够捕捉单词之间的语义相似性，使得相似的单词在向量空间中彼此接近
- 计算效率高：Word2Vec 使用浅层神经网络，计算效率较高，适合处理大规模语料
- 广泛应用：生成的词向量可以应用于各种 NLP 任务，如文本分类、情感分析、机器翻译等

>缺点
- 上下文无关：Word2Vec 生成的词向量是静态的，对于同一个单词在不同上下文中的含义无法区分
- 无法处理未登录词（OOV）：对于训练语料库中未出现的单词，Word2Vec 无法生成对应的词向量
- 需要大量数据：训练 Word2Vec 模型需要大量的文本数据，以确保生成的词向量质量较高

>总结
- Word2Vec 是一种有效的词向量生成技术，通过浅层神经网络捕捉单词之间的语义关系，在各种自然语言处理任务中表现出色。然而，它的静态表示和对上下文的忽略限制了其在某些复杂任务中的应用。随着技术的发展，基于上下文的词嵌入方法（如 BERT）逐渐成为研究的热点
