# word2vec

## 1 One-Hot
### 1.1 One-Hot 介绍
One-Hot 表示是一种将离散的单词或符号转换为向量的方法。在自然语言处理中，它将每个单词表示为一个向量，其中只有一个位置为1，其余位置为0

<div align=center>
    <image src="imgs/onehot.png" width=500>
</div>


### 1.2 One-Hot 表示的优缺点
>优点
- 简单易理解：One-Hot 表示非常直观且容易实现，不需要复杂的计算或预训练。
- 无先验假设：One-Hot 表示不引入任何关于单词之间关系的假设，因此不会引入偏差。
- 适用于分类问题：在某些分类任务中，One-Hot 表示能够很好地表示类别标签，如多分类问题中的目标变量。

>缺点
- 高维稀疏性：如果语料库很大，One-Hot 向量的维度将会变得非常高，且绝大多数位置为0，导致数据的稀疏性。这会增加存储和计算的负担
- 缺乏语义信息：One-Hot 向量无法捕捉单词之间的任何语义关系。不同的单词之间的距离（余弦相似度）总是相同的，这意味着“cat”和“dog”与“cat”和“fish”之间的相似度无法区分
- 不能处理未登录词（OOV）：如果测试数据中出现了训练集中没有的单词（未登录词），One-Hot 表示无法处理这些新词
- 无法处理上下文信息：One-Hot 表示是静态的，无法根据上下文调整单词的表示，限制了其在复杂语言任务中的应用。

>总结
- 虽然 One-Hot 表示在某些简单的任务中仍然有用，但由于其高维稀疏性和缺乏语义信息，现代自然语言处理方法更多地使用词向量（如 Word2Vec、GloVe）或上下文敏感的词嵌入（如 BERT）来表示单词和处理文本。这些方法能够更有效地捕捉单词之间的语义关系，并在各种 NLP 任务中表现出更好的性能


## 2 Word Vector 词向量

为了解决 One-Hot 存储和表达的问题，我们想一下，假如，可以如下表示：

<div align=center>
    <image src="imgs/wordvec.png" width=700>
</div>

也就是说，向量中的元素可以是任意实数，那么试想一下，是不是就可以不需要那么多维，都可以表示很多词？

那么，我们如何获得中间的这个表示词的矩阵呢？答案是 `word2vec。`


## 3 word2vec
Word2Vec 是一种用于生成词向量（Word Vectors）的技术，由 Google 的 Tomas Mikolov 等人在 2013 年提出。它通过浅层神经网络将单词映射到一个连续向量空间中，使得语义上相似的词在该空间中彼此接近。Word2Vec 主要有两种模型：连续词袋模型（CBOW）和跳字模型（Skip-gram）

### 3.1 模型原理
##### 连续词袋模型（Continuous Bag of Words, CBOW）
CBOW 模型通过上下文词（周围的词）来预测中心词。具体过程如下：
1. 给定一个中心词及其周围的上下文词。
2. 使用上下文词的向量表示，通过神经网络预测中心词。
3. 更新模型参数，使得模型能够更准确地预测中心词。

例如，给定句子 "The quick brown fox jumps over the lazy dog" 和中心词 "jumps"，CBOW 模型会使用上下文词 ["The", "quick", "brown", "fox", "over", "the", "lazy", "dog"] 来预测 "jumps"。

##### 跳字模型（Skip-gram）
Skip-gram 模型通过中心词来预测其周围的上下文词。具体过程如下：
1. 给定一个中心词
2. 使用中心词的向量表示，通过神经网络预测其周围的上下文词
3. 更新模型参数，使得模型能够更准确地预测上下文词

例如，给定句子 "The quick brown fox jumps over the lazy dog" 和中心词 "jumps"，Skip-gram 模型会使用 "jumps" 来预测上下文词 ["The", "quick", "brown", "fox", "over", "the", "lazy", "dog"]。

### 3.2 模型训练
Word2Vec 的训练过程主要包括以下步骤：

1. 语料准备：收集大量的文本数据，形成训练语料库
2. 构建词汇表：将语料库中的所有单词构建成词汇表，并为每个单词分配一个唯一的索引
3. 初始化词向量：初始化每个单词的向量表示，通常为随机值
4. 训练模型：使用 CBOW 或 Skip-gram 模型，通过梯度下降等优化算法，迭代调整词向量，使模型能够更好地预测上下文或中心词
5. 保存词向量：训练完成后，将词向量保存下来，以供后续使用

>优点
- 捕捉语义关系：Word2Vec 能够捕捉单词之间的语义相似性，使得相似的单词在向量空间中彼此接近
- 计算效率高：Word2Vec 使用浅层神经网络，计算效率较高，适合处理大规模语料
- 广泛应用：生成的词向量可以应用于各种 NLP 任务，如文本分类、情感分析、机器翻译等

>缺点
- 上下文无关：Word2Vec 生成的词向量是静态的，对于同一个单词在不同上下文中的含义无法区分
- 无法处理未登录词（OOV）：对于训练语料库中未出现的单词，Word2Vec 无法生成对应的词向量
- 需要大量数据：训练 Word2Vec 模型需要大量的文本数据，以确保生成的词向量质量较高

>总结
- Word2Vec 是一种有效的词向量生成技术，通过浅层神经网络捕捉单词之间的语义关系，在各种自然语言处理任务中表现出色。然而，它的静态表示和对上下文的忽略限制了其在某些复杂任务中的应用。随着技术的发展，基于上下文的词嵌入方法（如 BERT）逐渐成为研究的热点