# Transformer

Transformer 是一种基于注意力机制的深度学习模型架构，由 Ashish Vaswani 等人在 2017 年提出，其论文标题为“Attention Is All You Need”​

Transformer 模型与之前主流的循环神经网络（RNN）和卷积神经网络（CNN）不同，它完全依赖于注意力机制，完全去除了传统的递归和卷积操作。

## 1 Transformer 的基本构造
Transformer 的主要结构是 `编码器-解码器(Encoder-Decoder)` 架构。编码器将输入序列转换为连续表示，然后解码器基于这些表示生成输出序列。两者都使用多层的自注意力机制和全连接层来处理数据。

<div align=center>
    <image src="imgs/transformer-model.png" width=300>
</div>

### 1.1 自注意力机制（Self-Attention Mechanism）
自注意力机制是 Transformer 中的核心，它计算输入序列中各个位置之间的关系，从而为每个位置生成相应的输出表示。自注意力机制的优点在于它可以捕捉序列中不同位置之间的全局依赖关系，无论这些位置之间的距离有多远

<div align=center>
    <image src="imgs/scaled-dot-product-attention.png" width=200>
</div>

### 1.2 多头注意力（Multi-Head Attention）
Transformer 使用多头注意力机制，它通过对输入进行多次线性投影，生成多个不同的“头”（head），并行计算这些头的注意力，最后将结果连接起来。多头注意力允许模型从不同的子空间中获取信息，提高了模型的表达能力​(Vaswani et al. - Attent…)。

### 1.3 位置编码（Positional Encoding）
由于 Transformer 不使用递归或卷积结构，因此它没有内置的序列顺序信息。为此，Transformer 引入了位置编码，这是一种将序列中每个位置的位置信息显式编码到输入中的方法。通常，位置编码通过正弦和余弦函数来生成，模型可以利用这些编码推断出序列的顺序信息​

<div align=center>
    <image src="imgs/Multi-Head Attention.png" width=200>
</div>


## 2 Transformer 的优势
1. 并行化能力强：由于自注意力机制可以同时处理输入序列的所有位置，Transformer 的训练过程可以高度并行化，显著减少了训练时间。
2. 长程依赖捕捉能力强：通过自注意力机制，Transformer 可以更容易地捕捉序列中远距离位置之间的依赖关系，而不需要像 RNN 那样逐步传递信息。
3. 性能卓越：在机器翻译等任务中，Transformer 显示出卓越的性能，在标准基准测试中超过了当时的所有其他模型

总体而言，Transformer 的提出代表了自然语言处理（NLP）领域的一个重大进步，成为了之后许多模型的基础，例如 BERT、GPT 等。