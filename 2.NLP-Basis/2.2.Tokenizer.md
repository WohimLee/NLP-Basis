# Tokenizer
## 1 Tokenizer
Tokenizer 是自然语言处理 (NLP) 中的一种工具或过程，用来将文本分割为更小的基本单元，通常是单词、子词或字符。其主要目的是将一段连续的文本转换为结构化的形式，方便进一步的文本分析、处理或建模。具体来说，Tokenizer 可以根据不同的任务和应用场景，进行以下几种操作：

### 1.1 Tokenizer 的基本作用
1. 分词：将一段文本拆解成独立的词或子词。例如，对于英文句子 "I love natural language processing."，Tokenizer 可以将其分为 ["I", "love", "natural", "language", "processing", "."]
2. 字符级别分割：对于某些语言或应用场景，可以按字符进行分割。例如，对于中文句子 "我爱自然语言处理"，可以按字分为 ["我", "爱", "自然", "语言", "处理"]。
3. 子词分割：现代预训练模型（如 BERT 和 GPT 系列）常使用子词（subword）作为基本单元，以处理低频词和新词。这类分词器会将 "processing" 分割成 ["pro", "cess", "ing"]，从而提高模型对低频和未登录词的处理能力。

### 1.2 Tokenizer 的类型

>基于规则的 Tokenizer
- 这些 Tokenizer 使用特定的规则来分割文本，例如基于空格、标点符号等。常用于英文等分词较为简单的语言。
- 优点：简单快速，适用于不需要复杂处理的语言。
- 缺点：对如中文等没有天然词边界的语言不太适用。


>基于词典的 Tokenizer
- 使用预定义的词典或词库来匹配和分割单词。像 Jieba 等中文分词工具就是基于词典来进行分词的。
- 优点：适合处理已知的词汇，速度快。
- 缺点：对未登录词（新词）处理不够灵活，依赖词典的完整性。

>基于统计或机器学习的 Tokenizer
- 这些 Tokenizer 使用统计模型或机器学习算法来进行分词。例如条件随机场（CRF）或隐马尔可夫模型（HMM）可以根据上下文信息进行分词。
- 优点：可以更好地处理上下文依赖的词汇，并且在领域自适应时表现较好
- 缺点：训练和使用需要大量标注数据，训练过程复杂

>基于子词的 Tokenizer
- 现代 NLP 模型，如 BERT 和 GPT，通常使用子词分词方法，比如 Byte-Pair Encoding (BPE)、WordPiece 或 SentencePiece。这些方法能够将一个单词切分为更小的子词单元，减少词汇量并提高模型对未登录词的处理能力。
- 优点：处理未登录词和低频词表现优异
- 缺点：生成的子词不一定直观，分割可能会引入一定的复杂性


## 2 常见的中文 Tokenizer
在中文自然语言处理中，tokenization 是非常关键的一步，因为中文不像英文有空格作为天然的分词边界。常用的中文分词工具有很多，以下是一些常见的中文 tokenizer，以及如何训练自己的 tokenizer。


>Jieba
- Jieba 是最为常用的中文分词工具之一，基于字典和 HMM（隐马尔可夫模型）进行分词，支持精确模式、全模式和搜索引擎模式
- 优点：使用简单，支持自定义词典，速度较快
- 缺点：对于特定领域或者新词，可能表现不佳


>BERT-based Tokenizer (Subword Tokenization)
- 针对现代深度学习模型，像 BERT、GPT 等预训练模型通常使用子词（subword）分词，例如 WordPiece 或 SentencePiece
- 优点：对低频词和新词有很好的泛化能力，能够处理未登录词
- 缺点：可能比传统的分词方法更复杂且速度较慢