
# word2vec
Word2Vec 是一种用于生成词向量（Word Vectors）的技术，由 Google 的 Tomas Mikolov 等人在 2013 年提出。它通过浅层神经网络将单词映射到一个连续向量空间中，使得语义上相似的词在该空间中彼此接近。Word2Vec 主要有两种模型：
- 连续词袋模型（CBOW）
- 跳字模型（Skip-gram）

word2vec的重要假设：文本中离的越近的词，相似度越高

上下文词：给定一个窗口，例如说 2，那么上下文词就是前后一共2个词

## 1 Intro


## 2 模型原理

### 1.1 连续词袋模型（Continuous Bag of Words, CBOW）
CBOW 模型通过上下文词（周围的词）来预测中心词。

>具体过程
1. 给定一个中心词及其周围的上下文词。
2. 使用上下文词的向量表示，通过神经网络预测中心词。
3. 更新模型参数，使得模型能够更准确地预测中心词。

例如，给定句子 "The quick brown fox jumps over the lazy dog" 和中心词 "jumps"，CBOW 模型会使用上下文词 ["The", "quick", "brown", "fox", "over", "the", "lazy", "dog"] 来预测 "jumps"。

<div align=center>
    <image src="imgs/CBOW.png" width=600>
</div>

### 1.2 跳字模型（Skip-gram）
Skip-gram 模型通过中心词来预测其周围的上下文词。

>具体过程
1. 给定一个中心词
2. 使用中心词的向量表示，通过神经网络预测其周围的上下文词
3. 更新模型参数，使得模型能够更准确地预测上下文词

例如，给定句子 "The quick brown fox jumps over the lazy dog" 和中心词 "jumps"，Skip-gram 模型会使用 "jumps" 来预测上下文词 ["The", "quick", "brown", "fox", "over", "the", "lazy", "dog"]

<div align=center>
    <image src="imgs/skip-gram.png" width=600>
</div>

## 2 模型训练
Word2Vec 的训练过程主要包括以下步骤：

1. 语料准备：收集大量的文本数据，形成训练语料库
2. 构建词汇表：将语料库中的所有单词构建成词汇表，并为每个单词分配一个唯一的索引
3. 初始化词向量：初始化每个单词的向量表示，通常为随机值
4. 训练模型：使用 CBOW 或 Skip-gram 模型，通过梯度下降等优化算法，迭代调整词向量，使模型能够更好地预测上下文或中心词
5. 保存词向量：训练完成后，将词向量保存下来，以供后续使用

>优点
- 捕捉语义关系：Word2Vec 能够捕捉单词之间的语义相似性，使得相似的单词在向量空间中彼此接近
- 计算效率高：Word2Vec 使用浅层神经网络，计算效率较高，适合处理大规模语料
- 广泛应用：生成的词向量可以应用于各种 NLP 任务，如文本分类、情感分析、机器翻译等

>缺点
- 上下文无关：Word2Vec 生成的词向量是静态的，对于同一个单词在不同上下文中的含义无法区分
- 无法处理未登录词（OOV）：对于训练语料库中未出现的单词，Word2Vec 无法生成对应的词向量
- 需要大量数据：训练 Word2Vec 模型需要大量的文本数据，以确保生成的词向量质量较高

>总结
- Word2Vec 是一种有效的词向量生成技术，通过浅层神经网络捕捉单词之间的语义关系，在各种自然语言处理任务中表现出色。然而，它的静态表示和对上下文的忽略限制了其在某些复杂任务中的应用。随着技术的发展，基于上下文的词嵌入方法（如 BERT）逐渐成为研究的热点

## 3 负采样

